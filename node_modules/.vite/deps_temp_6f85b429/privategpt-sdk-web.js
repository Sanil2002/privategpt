import {
  ChatBody,
  ChunksBody,
  ChunksResponse,
  CompletionsBody,
  EmbeddingsBody,
  EmbeddingsResponse,
  HealthResponse,
  HttpValidationError,
  IngestResponse,
  IngestTextBody,
  OpenAiCompletion,
  Stream,
  SummarizeBody,
  SummarizeResponse,
  Supplier,
  fetcher
} from "./chunk-V5ZIWTOA.js";
import {
  StreamingTextResponse,
  streamToReadableStream
} from "./chunk-Y5NHQ5NN.js";
import {
  __commonJS,
  __export,
  __publicField,
  __toESM
} from "./chunk-Q4XP6UTR.js";

// node_modules/url-join/lib/url-join.js
var require_url_join = __commonJS({
  "node_modules/url-join/lib/url-join.js"(exports, module) {
    (function(name, context, definition) {
      if (typeof module !== "undefined" && module.exports) module.exports = definition();
      else if (typeof define === "function" && define.amd) define(definition);
      else context[name] = definition();
    })("urljoin", exports, function() {
      function normalize(strArray) {
        var resultArray = [];
        if (strArray.length === 0) {
          return "";
        }
        if (typeof strArray[0] !== "string") {
          throw new TypeError("Url must be a string. Received " + strArray[0]);
        }
        if (strArray[0].match(/^[^/:]+:\/*$/) && strArray.length > 1) {
          var first = strArray.shift();
          strArray[0] = first + strArray[0];
        }
        if (strArray[0].match(/^file:\/\/\//)) {
          strArray[0] = strArray[0].replace(/^([^/:]+):\/*/, "$1:///");
        } else {
          strArray[0] = strArray[0].replace(/^([^/:]+):\/*/, "$1://");
        }
        for (var i = 0; i < strArray.length; i++) {
          var component = strArray[i];
          if (typeof component !== "string") {
            throw new TypeError("Url must be a string. Received " + component);
          }
          if (component === "") {
            continue;
          }
          if (i > 0) {
            component = component.replace(/^[\/]+/, "");
          }
          if (i < strArray.length - 1) {
            component = component.replace(/[\/]+$/, "");
          } else {
            component = component.replace(/[\/]+$/, "/");
          }
          resultArray.push(component);
        }
        var str = resultArray.join("/");
        str = str.replace(/\/(\?|&|#[^!])/g, "$1");
        var parts = str.split("?");
        str = parts.shift() + (parts.length > 0 ? "?" : "") + parts.join("&");
        return str;
      }
      return function() {
        var input;
        if (typeof arguments[0] === "object") {
          input = arguments[0];
        } else {
          input = [].slice.call(arguments);
        }
        return normalize(input);
      };
    });
  }
});

// node_modules/privategpt-sdk-utils/api/index.js
var api_exports = {};
__export(api_exports, {
  OpenAiCompletionObject: () => OpenAiCompletionObject,
  OpenAiMessageRole: () => OpenAiMessageRole,
  Recipes: () => Recipes,
  UnprocessableEntityError: () => UnprocessableEntityError,
  contextChunks: () => contextChunks_exports,
  contextualCompletions: () => contextualCompletions_exports,
  embeddings: () => embeddings_exports,
  health: () => health_exports,
  ingestion: () => ingestion_exports
});

// node_modules/privategpt-sdk-utils/api/resources/embeddings/index.js
var embeddings_exports = {};

// node_modules/privategpt-sdk-utils/api/resources/contextualCompletions/index.js
var contextualCompletions_exports = {};

// node_modules/privategpt-sdk-utils/api/resources/contextChunks/index.js
var contextChunks_exports = {};

// node_modules/privategpt-sdk-utils/api/resources/ingestion/index.js
var ingestion_exports = {};

// node_modules/privategpt-sdk-utils/api/resources/health/index.js
var health_exports = {};

// node_modules/privategpt-sdk-utils/errors/PrivategptApiError.js
var PrivategptApiError = class _PrivategptApiError extends Error {
  constructor({ message, statusCode, body }) {
    super(buildMessage({ message, statusCode, body }));
    __publicField(this, "statusCode");
    __publicField(this, "body");
    Object.setPrototypeOf(this, _PrivategptApiError.prototype);
    if (statusCode != null) {
      this.statusCode = statusCode;
    }
    if (body !== void 0) {
      this.body = body;
    }
  }
};
function buildMessage({ message, statusCode, body }) {
  let lines = [];
  if (message != null) {
    lines.push(message);
  }
  if (statusCode != null) {
    lines.push(`Status code: ${statusCode.toString()}`);
  }
  if (body != null) {
    lines.push(`Body: ${JSON.stringify(body, void 0, 2)}`);
  }
  return lines.join("\n");
}

// node_modules/privategpt-sdk-utils/errors/PrivategptApiTimeoutError.js
var PrivategptApiTimeoutError = class _PrivategptApiTimeoutError extends Error {
  constructor() {
    super("Timeout");
    Object.setPrototypeOf(this, _PrivategptApiTimeoutError.prototype);
  }
};

// node_modules/privategpt-sdk-utils/api/resources/recipes/summarize/client/Client.js
var import_url_join = __toESM(require_url_join());
var Summarize = class {
  constructor(_options) {
    __publicField(this, "_options");
    this._options = _options;
  }
  /**
   * Given a list of messages comprising a conversation, return a response.
   *
   * Optionally include an initial `role: system` message to influence the way
   * the LLM answers.
   *
   * If `use_context` is set to `true`, the model will use context coming
   * from the ingested documents to create the response. The documents being used can
   * be filtered using the `context_filter` and passing the document IDs to be used.
   * Ingested documents IDs can be found using `/ingest/list` endpoint. If you want
   * all ingested documents to be used, remove `context_filter` altogether.
   *
   * When using `'include_sources': true`, the API will return the source Chunks used
   * to create the response, which come from the context provided.
   *
   * ```
   * {"id":"12345","object":"completion.chunk","created":1694268190,
   * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
   * "finish_reason":null}]}
   * ```
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.contextualCompletions.chatCompletion({
   *         messages: [],
   *         contextFilter: {}
   *     })
   */
  async summarizeStream(request, requestOptions, abortSignal) {
    const _response = await fetcher({
      abortSignal,
      url: (0, import_url_join.default)(await Supplier.get(this._options.environment), "v1/summarize"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      body: await SummarizeBody.jsonOrThrow({ ...request, stream: true }, {
        unrecognizedObjectKeys: "strip"
      }),
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries,
      responseType: "streaming"
    });
    if (_response.ok) {
      const stream = new Stream({
        // @ts-ignore
        stream: _response.body,
        terminator: "\n",
        parse: async (data) => {
          return await OpenAiCompletion.parseOrThrow(data, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            skipValidation: true,
            breadcrumbsPrefix: ["response"]
          });
        }
      });
      return stream;
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
  async summarize(request, requestOptions) {
    const _response = await fetcher({
      url: (0, import_url_join.default)(await Supplier.get(this._options.environment), "v1/summarize"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      body: await SummarizeBody.jsonOrThrow(request, {
        unrecognizedObjectKeys: "strip"
      }),
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries,
      responseType: "json"
    });
    if (_response.ok) {
      return await SummarizeResponse.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
};

// node_modules/privategpt-sdk-utils/api/resources/recipes/Client.js
var Recipes = class {
  constructor(_options) {
    __publicField(this, "_options");
    __publicField(this, "_summarize");
    this._options = _options;
  }
  get summarize() {
    return this._summarize ?? (this._summarize = new Summarize(this._options));
  }
};

// node_modules/privategpt-sdk-utils/api/types/OpenAiCompletionObject.js
var OpenAiCompletionObject = {
  Completion: "completion",
  CompletionChunk: "completion.chunk"
};

// node_modules/privategpt-sdk-utils/api/types/OpenAiMessageRole.js
var OpenAiMessageRole = {
  Assistant: "assistant",
  System: "system",
  User: "user"
};

// node_modules/privategpt-sdk-utils/api/errors/UnprocessableEntityError.js
var UnprocessableEntityError = class _UnprocessableEntityError extends PrivategptApiError {
  constructor(body) {
    super({
      message: "UnprocessableEntityError",
      statusCode: 422,
      body
    });
    Object.setPrototypeOf(this, _UnprocessableEntityError.prototype);
  }
};

// node_modules/privategpt-sdk-utils/api/resources/contextChunks/client/Client.js
var import_url_join2 = __toESM(require_url_join());
var ContextChunks = class {
  constructor(_options) {
    __publicField(this, "_options");
    this._options = _options;
  }
  /**
   * Given a `text`, returns the most relevant chunks from the ingested documents.
   *
   * The returned information can be used to generate prompts that can be
   * passed to `/completions` or `/chat/completions` APIs. Note: it is usually a very
   * fast API, because only the Embeddings model is involved, not the LLM. The
   * returned information contains the relevant chunk `text` together with the source
   * `document` it is coming from. It also contains a score that can be used to
   * compare different results.
   *
   * The max number of chunks to be returned is set using the `limit` param.
   *
   * Previous and next chunks (pieces of text that appear right before or after in the
   * document) can be fetched by using the `prev_next_chunks` field.
   *
   * The documents being used can be filtered using the `context_filter` and passing
   * the document IDs to be used. Ingested documents IDs can be found using
   * `/ingest/list` endpoint. If you want all ingested documents to be used,
   * remove `context_filter` altogether.
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.contextChunks.chunksRetrieval({
   *         text: "string",
   *         contextFilter: {}
   *     })
   */
  async chunksRetrieval(request, requestOptions) {
    const _response = await fetcher({
      url: (0, import_url_join2.default)(await Supplier.get(this._options.environment), "v1/chunks"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      body: await ChunksBody.jsonOrThrow(request, {
        unrecognizedObjectKeys: "strip"
      }),
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries
    });
    if (_response.ok) {
      return await ChunksResponse.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
};

// node_modules/privategpt-sdk-utils/api/resources/contextualCompletions/client/Client.js
var import_url_join3 = __toESM(require_url_join());
var ContextualCompletions = class {
  constructor(_options) {
    __publicField(this, "_options");
    this._options = _options;
  }
  /**
   * We recommend most users use our Chat completions API.
   *
   * Given a prompt, the model will return one predicted completion.
   *
   * Optionally include a `system_prompt` to influence the way the LLM answers.
   *
   * If `use_context`
   * is set to `true`, the model will use context coming from the ingested documents
   * to create the response. The documents being used can be filtered using the
   * `context_filter` and passing the document IDs to be used. Ingested documents IDs
   * can be found using `/ingest/list` endpoint. If you want all ingested documents to
   * be used, remove `context_filter` altogether.
   *
   * When using `'include_sources': true`, the API will return the source Chunks used
   * to create the response, which come from the context provided.
   *
   * ```
   * {"id":"12345","object":"completion.chunk","created":1694268190,
   * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
   * "finish_reason":null}]}
   * ```
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.contextualCompletions.promptCompletionStream({
   *         prompt: "string",
   *         contextFilter: {}
   *     })
   */
  async promptCompletionStream(request, requestOptions, abortSignal) {
    const _response = await fetcher({
      abortSignal,
      url: (0, import_url_join3.default)(await Supplier.get(this._options.environment), "v1/completions"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      body: await CompletionsBody.jsonOrThrow({ ...request, stream: true }, {
        unrecognizedObjectKeys: "strip"
      }),
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries,
      responseType: "streaming"
    });
    if (_response.ok) {
      const stream = new Stream({
        // @ts-ignore
        stream: _response.body,
        terminator: "\n",
        parse: async (data) => {
          return await OpenAiCompletion.parseOrThrow(data, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            skipValidation: true,
            breadcrumbsPrefix: ["response"]
          });
        }
      });
      return stream;
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
  /**
   * We recommend most users use our Chat completions API.
   *
   * Given a prompt, the model will return one predicted completion.
   *
   * Optionally include a `system_prompt` to influence the way the LLM answers.
   *
   * If `use_context`
   * is set to `true`, the model will use context coming from the ingested documents
   * to create the response. The documents being used can be filtered using the
   * `context_filter` and passing the document IDs to be used. Ingested documents IDs
   * can be found using `/ingest/list` endpoint. If you want all ingested documents to
   * be used, remove `context_filter` altogether.
   *
   * When using `'include_sources': true`, the API will return the source Chunks used
   * to create the response, which come from the context provided.
   *
   * ```
   * {"id":"12345","object":"completion.chunk","created":1694268190,
   * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
   * "finish_reason":null}]}
   * ```
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.contextualCompletions.promptCompletion({
   *         prompt: "string",
   *         contextFilter: {}
   *     })
   */
  async promptCompletion(request, requestOptions) {
    const _response = await fetcher({
      url: (0, import_url_join3.default)(await Supplier.get(this._options.environment), "v1/completions"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      body: await CompletionsBody.jsonOrThrow(request, {
        unrecognizedObjectKeys: "strip"
      }),
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries,
      responseType: "json"
    });
    if (_response.ok) {
      return await OpenAiCompletion.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
  /**
   * Given a list of messages comprising a conversation, return a response.
   *
   * Optionally include an initial `role: system` message to influence the way
   * the LLM answers.
   *
   * If `use_context` is set to `true`, the model will use context coming
   * from the ingested documents to create the response. The documents being used can
   * be filtered using the `context_filter` and passing the document IDs to be used.
   * Ingested documents IDs can be found using `/ingest/list` endpoint. If you want
   * all ingested documents to be used, remove `context_filter` altogether.
   *
   * When using `'include_sources': true`, the API will return the source Chunks used
   * to create the response, which come from the context provided.
   *
   * ```
   * {"id":"12345","object":"completion.chunk","created":1694268190,
   * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
   * "finish_reason":null}]}
   * ```
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.contextualCompletions.chatCompletion({
   *         messages: [],
   *         contextFilter: {}
   *     })
   */
  async chatCompletionStream(request, requestOptions, abortSignal) {
    const _response = await fetcher({
      abortSignal,
      url: (0, import_url_join3.default)(await Supplier.get(this._options.environment), "v1/chat/completions"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      body: await ChatBody.jsonOrThrow({ ...request, stream: true }, {
        unrecognizedObjectKeys: "strip"
      }),
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries,
      responseType: "streaming"
    });
    if (_response.ok) {
      const stream = new Stream({
        // @ts-ignore
        stream: _response.body,
        terminator: "\n",
        parse: async (data) => {
          return await OpenAiCompletion.parseOrThrow(data, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            skipValidation: true,
            breadcrumbsPrefix: ["response"]
          });
        }
      });
      return stream;
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
  async chatCompletion(request, requestOptions) {
    const _response = await fetcher({
      url: (0, import_url_join3.default)(await Supplier.get(this._options.environment), "v1/chat/completions"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      body: await ChatBody.jsonOrThrow(request, {
        unrecognizedObjectKeys: "strip"
      }),
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries,
      responseType: "json"
    });
    if (_response.ok) {
      return await OpenAiCompletion.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
};

// node_modules/privategpt-sdk-utils/api/resources/embeddings/client/Client.js
var import_url_join4 = __toESM(require_url_join());
var Embeddings = class {
  constructor(_options) {
    __publicField(this, "_options");
    this._options = _options;
  }
  /**
   * Get a vector representation of a given input.
   *
   * That vector representation can be easily consumed
   * by machine learning models and algorithms.
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.embeddings.embeddingsGeneration({})
   */
  async embeddingsGeneration(request, requestOptions) {
    const _response = await fetcher({
      url: (0, import_url_join4.default)(await Supplier.get(this._options.environment), "v1/embeddings"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      body: await EmbeddingsBody.jsonOrThrow(request, {
        unrecognizedObjectKeys: "strip"
      }),
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries
    });
    if (_response.ok) {
      return await EmbeddingsResponse.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
};

// node_modules/privategpt-sdk-utils/api/resources/health/client/Client.js
var import_url_join5 = __toESM(require_url_join());
var Health = class {
  constructor(_options) {
    __publicField(this, "_options");
    this._options = _options;
  }
  /**
   * Return ok if the system is up.
   *
   * @example
   *     await privategptApi.health.health()
   */
  async health(requestOptions) {
    const _response = await fetcher({
      url: (0, import_url_join5.default)(await Supplier.get(this._options.environment), "health"),
      method: "GET",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries
    });
    if (_response.ok) {
      return await HealthResponse.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      throw new PrivategptApiError({
        statusCode: _response.error.statusCode,
        body: _response.error.body
      });
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
};

// node_modules/privategpt-sdk-utils/api/resources/ingestion/client/Client.js
var import_url_join6 = __toESM(require_url_join());
var Ingestion = class {
  constructor(_options) {
    __publicField(this, "_options");
    this._options = _options;
  }
  /**
   * Ingests and processes a file.
   *
   * Deprecated. Use ingest/file instead.
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   */
  async ingest(file, requestOptions) {
    const _request = new FormData();
    _request.append("file", file);
    const _response = await fetcher({
      url: (0, import_url_join6.default)(await Supplier.get(this._options.environment), "v1/ingest"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "multipart/form-data; boundary=" + file.size,
      body: _request,
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries
    });
    if (_response.ok) {
      return await IngestResponse.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
  /**
   * Ingests and processes a file, storing its chunks to be used as context.
   *
   * The context obtained from files is later used in
   * `/chat/completions`, `/completions`, and `/chunks` APIs.
   *
   * Most common document
   * formats are supported, but you may be prompted to install an extra dependency to
   * manage a specific file type.
   *
   * A file can generate different Documents (for example a PDF generates one Document
   * per page). All Documents IDs are returned in the response, together with the
   * extracted Metadata (which is later used to improve context retrieval). Those IDs
   * can be used to filter the context used to create responses in
   * `/chat/completions`, `/completions`, and `/chunks` APIs.
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   */
  async ingestFile(file, requestOptions) {
    const _request = new FormData();
    _request.append("file", file);
    const _response = await fetcher({
      url: (0, import_url_join6.default)(await Supplier.get(this._options.environment), "v1/ingest/file"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      // contentType: 'multipart/form-data; boundary=' + _request.getBoundary(),
      body: _request,
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries
    });
    if (_response.ok) {
      return await IngestResponse.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
  /**
   * Ingests and processes a text, storing its chunks to be used as context.
   *
   * The context obtained from files is later used in
   * `/chat/completions`, `/completions`, and `/chunks` APIs.
   *
   * A Document will be generated with the given text. The Document
   * ID is returned in the response, together with the
   * extracted Metadata (which is later used to improve context retrieval). That ID
   * can be used to filter the context used to create responses in
   * `/chat/completions`, `/completions`, and `/chunks` APIs.
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.ingestion.ingestText({
   *         fileName: "string",
   *         text: "string"
   *     })
   */
  async ingestText(request, requestOptions) {
    const _response = await fetcher({
      url: (0, import_url_join6.default)(await Supplier.get(this._options.environment), "v1/ingest/text"),
      method: "POST",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      body: await IngestTextBody.jsonOrThrow(request, {
        unrecognizedObjectKeys: "strip"
      }),
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries
    });
    if (_response.ok) {
      return await IngestResponse.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
  /**
   * Lists already ingested Documents including their Document ID and metadata.
   *
   * Those IDs can be used to filter the context used to create responses
   * in `/chat/completions`, `/completions`, and `/chunks` APIs.
   *
   * @example
   *     await privategptApi.ingestion.listIngested()
   */
  async listIngested(requestOptions) {
    const _response = await fetcher({
      url: (0, import_url_join6.default)(await Supplier.get(this._options.environment), "v1/ingest/list"),
      method: "GET",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries
    });
    if (_response.ok) {
      return await IngestResponse.parseOrThrow(_response.body, {
        unrecognizedObjectKeys: "passthrough",
        allowUnrecognizedUnionMembers: true,
        allowUnrecognizedEnumValues: true,
        breadcrumbsPrefix: ["response"]
      });
    }
    if (_response.error.reason === "status-code") {
      throw new PrivategptApiError({
        statusCode: _response.error.statusCode,
        body: _response.error.body
      });
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
  /**
   * Delete the specified ingested Document.
   *
   * The `doc_id` can be obtained from the `GET /ingest/list` endpoint.
   * The document will be effectively deleted from your storage context.
   * @throws {@link PrivategptApi.UnprocessableEntityError}
   *
   * @example
   *     await privategptApi.ingestion.deleteIngested("string")
   */
  async deleteIngested(docId, requestOptions) {
    const _response = await fetcher({
      url: (0, import_url_join6.default)(await Supplier.get(this._options.environment), `v1/ingest/${docId}`),
      method: "DELETE",
      headers: {
        "X-Fern-Language": "JavaScript"
      },
      contentType: "application/json",
      timeoutMs: (requestOptions == null ? void 0 : requestOptions.timeoutInSeconds) != null ? requestOptions.timeoutInSeconds * 1e3 : 6e4,
      maxRetries: requestOptions == null ? void 0 : requestOptions.maxRetries
    });
    if (_response.ok) {
      return _response.body;
    }
    if (_response.error.reason === "status-code") {
      switch (_response.error.statusCode) {
        case 422:
          throw new UnprocessableEntityError(await HttpValidationError.parseOrThrow(_response.error.body, {
            unrecognizedObjectKeys: "passthrough",
            allowUnrecognizedUnionMembers: true,
            allowUnrecognizedEnumValues: true,
            breadcrumbsPrefix: ["response"]
          }));
        default:
          throw new PrivategptApiError({
            statusCode: _response.error.statusCode,
            body: _response.error.body
          });
      }
    }
    switch (_response.error.reason) {
      case "non-json":
        throw new PrivategptApiError({
          statusCode: _response.error.statusCode,
          body: _response.error.rawBody
        });
      case "timeout":
        throw new PrivategptApiTimeoutError();
      case "unknown":
        throw new PrivategptApiError({
          message: _response.error.errorMessage
        });
    }
  }
};

// node_modules/privategpt-sdk-utils/Client.js
var PrivategptApiClient = class {
  constructor(_options) {
    __publicField(this, "_options");
    __publicField(this, "_contextualCompletions");
    __publicField(this, "_contextChunks");
    __publicField(this, "_ingestion");
    __publicField(this, "_embeddings");
    __publicField(this, "_recipes");
    __publicField(this, "_health");
    this._options = _options;
  }
  get contextualCompletions() {
    return this._contextualCompletions ?? (this._contextualCompletions = new ContextualCompletions(this._options));
  }
  get contextChunks() {
    return this._contextChunks ?? (this._contextChunks = new ContextChunks(this._options));
  }
  get ingestion() {
    return this._ingestion ?? (this._ingestion = new Ingestion(this._options));
  }
  get embeddings() {
    return this._embeddings ?? (this._embeddings = new Embeddings(this._options));
  }
  get recipes() {
    return this._recipes ?? (this._recipes = new Recipes(this._options));
  }
  get health() {
    return this._health ?? (this._health = new Health(this._options));
  }
};
export {
  api_exports as PrivategptApi,
  PrivategptApiClient,
  PrivategptApiError,
  PrivategptApiTimeoutError,
  StreamingTextResponse,
  streamToReadableStream
};
//# sourceMappingURL=privategpt-sdk-web.js.map
