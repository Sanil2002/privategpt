/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as PrivategptApi from '../../../..';
import * as core from '../../../../../core';
export declare namespace Summarize {
    interface Options {
        environment: core.Supplier<string>;
    }
    interface RequestOptions {
        timeoutInSeconds?: number;
        maxRetries?: number;
    }
}
declare type SummarizeBody = Omit<PrivategptApi.SummarizeBody, 'stream'>;
declare type RequestOptionsStreaming = Omit<Summarize.RequestOptions, 'stream'>;
declare type RequestOptionsNotStreaming = Omit<Summarize.RequestOptions, 'stream'> & {
    stream: false;
};
export declare class Summarize {
    protected readonly _options: Summarize.Options;
    constructor(_options: Summarize.Options);
    /**
     * Given a list of messages comprising a conversation, return a response.
     *
     * Optionally include an initial `role: system` message to influence the way
     * the LLM answers.
     *
     * If `use_context` is set to `true`, the model will use context coming
     * from the ingested documents to create the response. The documents being used can
     * be filtered using the `context_filter` and passing the document IDs to be used.
     * Ingested documents IDs can be found using `/ingest/list` endpoint. If you want
     * all ingested documents to be used, remove `context_filter` altogether.
     *
     * When using `'include_sources': true`, the API will return the source Chunks used
     * to create the response, which come from the context provided.
     *
     * ```
     * {"id":"12345","object":"completion.chunk","created":1694268190,
     * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
     * "finish_reason":null}]}
     * ```
     * @throws {@link PrivategptApi.UnprocessableEntityError}
     *
     * @example
     *     await privategptApi.contextualCompletions.chatCompletion({
     *         messages: [],
     *         contextFilter: {}
     *     })
     */
    summarizeStream(request: SummarizeBody, requestOptions?: Summarize.RequestOptions, abortSignal?: AbortSignal): Promise<core.Stream<PrivategptApi.OpenAiCompletion>>;
    summarize(request: SummarizeBody, requestOptions?: RequestOptionsNotStreaming | RequestOptionsStreaming): Promise<PrivategptApi.SummarizeResponse>;
}
export {};
