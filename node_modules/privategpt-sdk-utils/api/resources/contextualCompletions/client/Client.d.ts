/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as PrivategptApi from '../../..';
import * as core from '../../../../core';
export declare namespace ContextualCompletions {
    interface Options {
        environment: core.Supplier<string>;
    }
    interface RequestOptions {
        timeoutInSeconds?: number;
        maxRetries?: number;
    }
}
declare type ChatBody = Omit<PrivategptApi.ChatBody, 'stream'>;
declare type PromptBody = Omit<PrivategptApi.CompletionsBody, 'stream'>;
declare type RequestOptionsStreaming = Omit<ContextualCompletions.RequestOptions, 'stream'>;
declare type RequestOptionsNotStreaming = Omit<ContextualCompletions.RequestOptions, 'stream'> & {
    stream: false;
};
export declare class ContextualCompletions {
    protected readonly _options: ContextualCompletions.Options;
    constructor(_options: ContextualCompletions.Options);
    /**
     * We recommend most users use our Chat completions API.
     *
     * Given a prompt, the model will return one predicted completion.
     *
     * Optionally include a `system_prompt` to influence the way the LLM answers.
     *
     * If `use_context`
     * is set to `true`, the model will use context coming from the ingested documents
     * to create the response. The documents being used can be filtered using the
     * `context_filter` and passing the document IDs to be used. Ingested documents IDs
     * can be found using `/ingest/list` endpoint. If you want all ingested documents to
     * be used, remove `context_filter` altogether.
     *
     * When using `'include_sources': true`, the API will return the source Chunks used
     * to create the response, which come from the context provided.
     *
     * ```
     * {"id":"12345","object":"completion.chunk","created":1694268190,
     * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
     * "finish_reason":null}]}
     * ```
     * @throws {@link PrivategptApi.UnprocessableEntityError}
     *
     * @example
     *     await privategptApi.contextualCompletions.promptCompletionStream({
     *         prompt: "string",
     *         contextFilter: {}
     *     })
     */
    promptCompletionStream(request: PromptBody, requestOptions?: ContextualCompletions.RequestOptions, abortSignal?: AbortSignal): Promise<core.Stream<PrivategptApi.OpenAiCompletion>>;
    /**
     * We recommend most users use our Chat completions API.
     *
     * Given a prompt, the model will return one predicted completion.
     *
     * Optionally include a `system_prompt` to influence the way the LLM answers.
     *
     * If `use_context`
     * is set to `true`, the model will use context coming from the ingested documents
     * to create the response. The documents being used can be filtered using the
     * `context_filter` and passing the document IDs to be used. Ingested documents IDs
     * can be found using `/ingest/list` endpoint. If you want all ingested documents to
     * be used, remove `context_filter` altogether.
     *
     * When using `'include_sources': true`, the API will return the source Chunks used
     * to create the response, which come from the context provided.
     *
     * ```
     * {"id":"12345","object":"completion.chunk","created":1694268190,
     * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
     * "finish_reason":null}]}
     * ```
     * @throws {@link PrivategptApi.UnprocessableEntityError}
     *
     * @example
     *     await privategptApi.contextualCompletions.promptCompletion({
     *         prompt: "string",
     *         contextFilter: {}
     *     })
     */
    promptCompletion(request: PromptBody, requestOptions?: ContextualCompletions.RequestOptions): Promise<PrivategptApi.OpenAiCompletion>;
    /**
     * Given a list of messages comprising a conversation, return a response.
     *
     * Optionally include an initial `role: system` message to influence the way
     * the LLM answers.
     *
     * If `use_context` is set to `true`, the model will use context coming
     * from the ingested documents to create the response. The documents being used can
     * be filtered using the `context_filter` and passing the document IDs to be used.
     * Ingested documents IDs can be found using `/ingest/list` endpoint. If you want
     * all ingested documents to be used, remove `context_filter` altogether.
     *
     * When using `'include_sources': true`, the API will return the source Chunks used
     * to create the response, which come from the context provided.
     *
     * ```
     * {"id":"12345","object":"completion.chunk","created":1694268190,
     * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
     * "finish_reason":null}]}
     * ```
     * @throws {@link PrivategptApi.UnprocessableEntityError}
     *
     * @example
     *     await privategptApi.contextualCompletions.chatCompletion({
     *         messages: [],
     *         contextFilter: {}
     *     })
     */
    chatCompletionStream(request: ChatBody, requestOptions?: ContextualCompletions.RequestOptions, abortSignal?: AbortSignal): Promise<core.Stream<PrivategptApi.OpenAiCompletion>>;
    chatCompletion(request: ChatBody, requestOptions?: RequestOptionsNotStreaming | RequestOptionsStreaming): Promise<PrivategptApi.OpenAiCompletion>;
}
export {};
