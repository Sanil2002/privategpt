/**
 * This file was auto-generated by Fern from our API Definition.
 */
import * as PrivategptApi from '../../..';
import * as core from '../../../../core';
import * as errors from '../../../../errors';
import * as serializers from '../../../../serialization';
import urlJoin from 'url-join';
export class ContextualCompletions {
    _options;
    constructor(_options) {
        this._options = _options;
    }
    /**
     * We recommend most users use our Chat completions API.
     *
     * Given a prompt, the model will return one predicted completion.
     *
     * Optionally include a `system_prompt` to influence the way the LLM answers.
     *
     * If `use_context`
     * is set to `true`, the model will use context coming from the ingested documents
     * to create the response. The documents being used can be filtered using the
     * `context_filter` and passing the document IDs to be used. Ingested documents IDs
     * can be found using `/ingest/list` endpoint. If you want all ingested documents to
     * be used, remove `context_filter` altogether.
     *
     * When using `'include_sources': true`, the API will return the source Chunks used
     * to create the response, which come from the context provided.
     *
     * ```
     * {"id":"12345","object":"completion.chunk","created":1694268190,
     * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
     * "finish_reason":null}]}
     * ```
     * @throws {@link PrivategptApi.UnprocessableEntityError}
     *
     * @example
     *     await privategptApi.contextualCompletions.promptCompletionStream({
     *         prompt: "string",
     *         contextFilter: {}
     *     })
     */
    async promptCompletionStream(request, requestOptions, abortSignal) {
        const _response = await core.fetcher({
            abortSignal,
            url: urlJoin(await core.Supplier.get(this._options.environment), 'v1/completions'),
            method: 'POST',
            headers: {
                'X-Fern-Language': 'JavaScript',
            },
            contentType: 'application/json',
            body: await serializers.CompletionsBody.jsonOrThrow({ ...request, stream: true }, {
                unrecognizedObjectKeys: 'strip',
            }),
            timeoutMs: requestOptions?.timeoutInSeconds != null
                ? requestOptions.timeoutInSeconds * 1000
                : 60000,
            maxRetries: requestOptions?.maxRetries,
            responseType: 'streaming',
        });
        if (_response.ok) {
            const stream = new core.Stream({
                // @ts-ignore
                stream: _response.body,
                terminator: '\n',
                parse: async (data) => {
                    return await serializers.OpenAiCompletion.parseOrThrow(data, {
                        unrecognizedObjectKeys: 'passthrough',
                        allowUnrecognizedUnionMembers: true,
                        allowUnrecognizedEnumValues: true,
                        skipValidation: true,
                        breadcrumbsPrefix: ['response'],
                    });
                },
            });
            return stream;
        }
        if (_response.error.reason === 'status-code') {
            switch (_response.error.statusCode) {
                case 422:
                    throw new PrivategptApi.UnprocessableEntityError(await serializers.HttpValidationError.parseOrThrow(_response.error.body, {
                        unrecognizedObjectKeys: 'passthrough',
                        allowUnrecognizedUnionMembers: true,
                        allowUnrecognizedEnumValues: true,
                        breadcrumbsPrefix: ['response'],
                    }));
                default:
                    throw new errors.PrivategptApiError({
                        statusCode: _response.error.statusCode,
                        body: _response.error.body,
                    });
            }
        }
        switch (_response.error.reason) {
            case 'non-json':
                throw new errors.PrivategptApiError({
                    statusCode: _response.error.statusCode,
                    body: _response.error.rawBody,
                });
            case 'timeout':
                throw new errors.PrivategptApiTimeoutError();
            case 'unknown':
                throw new errors.PrivategptApiError({
                    message: _response.error.errorMessage,
                });
        }
    }
    /**
     * We recommend most users use our Chat completions API.
     *
     * Given a prompt, the model will return one predicted completion.
     *
     * Optionally include a `system_prompt` to influence the way the LLM answers.
     *
     * If `use_context`
     * is set to `true`, the model will use context coming from the ingested documents
     * to create the response. The documents being used can be filtered using the
     * `context_filter` and passing the document IDs to be used. Ingested documents IDs
     * can be found using `/ingest/list` endpoint. If you want all ingested documents to
     * be used, remove `context_filter` altogether.
     *
     * When using `'include_sources': true`, the API will return the source Chunks used
     * to create the response, which come from the context provided.
     *
     * ```
     * {"id":"12345","object":"completion.chunk","created":1694268190,
     * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
     * "finish_reason":null}]}
     * ```
     * @throws {@link PrivategptApi.UnprocessableEntityError}
     *
     * @example
     *     await privategptApi.contextualCompletions.promptCompletion({
     *         prompt: "string",
     *         contextFilter: {}
     *     })
     */
    async promptCompletion(request, requestOptions) {
        const _response = await core.fetcher({
            url: urlJoin(await core.Supplier.get(this._options.environment), 'v1/completions'),
            method: 'POST',
            headers: {
                'X-Fern-Language': 'JavaScript',
            },
            contentType: 'application/json',
            body: await serializers.CompletionsBody.jsonOrThrow(request, {
                unrecognizedObjectKeys: 'strip',
            }),
            timeoutMs: requestOptions?.timeoutInSeconds != null
                ? requestOptions.timeoutInSeconds * 1000
                : 60000,
            maxRetries: requestOptions?.maxRetries,
            responseType: 'json',
        });
        if (_response.ok) {
            return await serializers.OpenAiCompletion.parseOrThrow(_response.body, {
                unrecognizedObjectKeys: 'passthrough',
                allowUnrecognizedUnionMembers: true,
                allowUnrecognizedEnumValues: true,
                breadcrumbsPrefix: ['response'],
            });
        }
        if (_response.error.reason === 'status-code') {
            switch (_response.error.statusCode) {
                case 422:
                    throw new PrivategptApi.UnprocessableEntityError(await serializers.HttpValidationError.parseOrThrow(_response.error.body, {
                        unrecognizedObjectKeys: 'passthrough',
                        allowUnrecognizedUnionMembers: true,
                        allowUnrecognizedEnumValues: true,
                        breadcrumbsPrefix: ['response'],
                    }));
                default:
                    throw new errors.PrivategptApiError({
                        statusCode: _response.error.statusCode,
                        body: _response.error.body,
                    });
            }
        }
        switch (_response.error.reason) {
            case 'non-json':
                throw new errors.PrivategptApiError({
                    statusCode: _response.error.statusCode,
                    body: _response.error.rawBody,
                });
            case 'timeout':
                throw new errors.PrivategptApiTimeoutError();
            case 'unknown':
                throw new errors.PrivategptApiError({
                    message: _response.error.errorMessage,
                });
        }
    }
    /**
     * Given a list of messages comprising a conversation, return a response.
     *
     * Optionally include an initial `role: system` message to influence the way
     * the LLM answers.
     *
     * If `use_context` is set to `true`, the model will use context coming
     * from the ingested documents to create the response. The documents being used can
     * be filtered using the `context_filter` and passing the document IDs to be used.
     * Ingested documents IDs can be found using `/ingest/list` endpoint. If you want
     * all ingested documents to be used, remove `context_filter` altogether.
     *
     * When using `'include_sources': true`, the API will return the source Chunks used
     * to create the response, which come from the context provided.
     *
     * ```
     * {"id":"12345","object":"completion.chunk","created":1694268190,
     * "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
     * "finish_reason":null}]}
     * ```
     * @throws {@link PrivategptApi.UnprocessableEntityError}
     *
     * @example
     *     await privategptApi.contextualCompletions.chatCompletion({
     *         messages: [],
     *         contextFilter: {}
     *     })
     */
    async chatCompletionStream(request, requestOptions, abortSignal) {
        const _response = await core.fetcher({
            abortSignal,
            url: urlJoin(await core.Supplier.get(this._options.environment), 'v1/chat/completions'),
            method: 'POST',
            headers: {
                'X-Fern-Language': 'JavaScript',
            },
            contentType: 'application/json',
            body: await serializers.ChatBody.jsonOrThrow({ ...request, stream: true }, {
                unrecognizedObjectKeys: 'strip',
            }),
            timeoutMs: requestOptions?.timeoutInSeconds != null
                ? requestOptions.timeoutInSeconds * 1000
                : 60000,
            maxRetries: requestOptions?.maxRetries,
            responseType: 'streaming',
        });
        if (_response.ok) {
            const stream = new core.Stream({
                // @ts-ignore
                stream: _response.body,
                terminator: '\n',
                parse: async (data) => {
                    return await serializers.OpenAiCompletion.parseOrThrow(data, {
                        unrecognizedObjectKeys: 'passthrough',
                        allowUnrecognizedUnionMembers: true,
                        allowUnrecognizedEnumValues: true,
                        skipValidation: true,
                        breadcrumbsPrefix: ['response'],
                    });
                },
            });
            return stream;
        }
        if (_response.error.reason === 'status-code') {
            switch (_response.error.statusCode) {
                case 422:
                    throw new PrivategptApi.UnprocessableEntityError(await serializers.HttpValidationError.parseOrThrow(_response.error.body, {
                        unrecognizedObjectKeys: 'passthrough',
                        allowUnrecognizedUnionMembers: true,
                        allowUnrecognizedEnumValues: true,
                        breadcrumbsPrefix: ['response'],
                    }));
                default:
                    throw new errors.PrivategptApiError({
                        statusCode: _response.error.statusCode,
                        body: _response.error.body,
                    });
            }
        }
        switch (_response.error.reason) {
            case 'non-json':
                throw new errors.PrivategptApiError({
                    statusCode: _response.error.statusCode,
                    body: _response.error.rawBody,
                });
            case 'timeout':
                throw new errors.PrivategptApiTimeoutError();
            case 'unknown':
                throw new errors.PrivategptApiError({
                    message: _response.error.errorMessage,
                });
        }
    }
    async chatCompletion(request, requestOptions) {
        const _response = await core.fetcher({
            url: urlJoin(await core.Supplier.get(this._options.environment), 'v1/chat/completions'),
            method: 'POST',
            headers: {
                'X-Fern-Language': 'JavaScript',
            },
            contentType: 'application/json',
            body: await serializers.ChatBody.jsonOrThrow(request, {
                unrecognizedObjectKeys: 'strip',
            }),
            timeoutMs: requestOptions?.timeoutInSeconds != null
                ? requestOptions.timeoutInSeconds * 1000
                : 60000,
            maxRetries: requestOptions?.maxRetries,
            responseType: 'json',
        });
        if (_response.ok) {
            return await serializers.OpenAiCompletion.parseOrThrow(_response.body, {
                unrecognizedObjectKeys: 'passthrough',
                allowUnrecognizedUnionMembers: true,
                allowUnrecognizedEnumValues: true,
                breadcrumbsPrefix: ['response'],
            });
        }
        if (_response.error.reason === 'status-code') {
            switch (_response.error.statusCode) {
                case 422:
                    throw new PrivategptApi.UnprocessableEntityError(await serializers.HttpValidationError.parseOrThrow(_response.error.body, {
                        unrecognizedObjectKeys: 'passthrough',
                        allowUnrecognizedUnionMembers: true,
                        allowUnrecognizedEnumValues: true,
                        breadcrumbsPrefix: ['response'],
                    }));
                default:
                    throw new errors.PrivategptApiError({
                        statusCode: _response.error.statusCode,
                        body: _response.error.body,
                    });
            }
        }
        switch (_response.error.reason) {
            case 'non-json':
                throw new errors.PrivategptApiError({
                    statusCode: _response.error.statusCode,
                    body: _response.error.rawBody,
                });
            case 'timeout':
                throw new errors.PrivategptApiTimeoutError();
            case 'unknown':
                throw new errors.PrivategptApiError({
                    message: _response.error.errorMessage,
                });
        }
    }
}
